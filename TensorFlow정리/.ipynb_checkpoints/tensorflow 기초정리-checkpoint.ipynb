{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f000532a",
   "metadata": {},
   "source": [
    "tensorflow 기초 및 함수 정리\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ad422",
   "metadata": {},
   "source": [
    "● One-hot (vector)\n",
    "\n",
    "데이터에 고유한 인덱스(숫자)를 붙여서 분류 \n",
    "\n",
    "자신이 가진 인덱스에서는 1, 나머지는 0\n",
    "\n",
    "ex) 딸기-0 사과-1 바나나-2 키위-3 수박-4\n",
    "\n",
    "사과 -> [0,1,0,0,0]\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● 상수 \n",
    "\n",
    "- tf.constant(): 상수 선언\n",
    "\n",
    "  ex) x=tf.constant(1) # 상수선언 x=1\n",
    "\n",
    "- tf.zeros(<shape>,<dtype>,<name>): 모든 원소의 값이 0인 텐서 생성\n",
    "\n",
    "  ex) tf.zeros([2,3], float32) => [ [0,0,0], [0,0,0] ]\n",
    "\n",
    "- tf.ones\n",
    "\n",
    "- tf.fill(<dims>,<value>,<name>): value 값으로 채워진 텐서를 생성함\n",
    "\n",
    "- tf.constant(<value>,<dytpe>,<shape>,<name>): 상수 텐서 생성\n",
    "\n",
    "\n",
    "- tf.range(<start>,<limit>,<delta>,<name>): start~limit 사이에서 delta 간격으로 뽑아낸 정수들의 리스트를 생성함\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● 난수 상수 생성\n",
    "\n",
    "- tf.random_normal(<shape>,<mean>,<stddev>,<dtype>,<seed>,<name>): 정규분포 난수를 생성\n",
    "\n",
    "   mean: 정규분포의 평균\n",
    "\n",
    "   stddev: 정규분포의 표준 편차\n",
    "\n",
    "- tf.random_uniform(<shape>,<minval>,<maxval>,<dtype>,<seed>,<name>): 균등분포 난수를 생성\n",
    "\n",
    "  minval: 최소값\n",
    "\n",
    " \n",
    "\n",
    "● 난수 텐서\n",
    "\n",
    "-  서로 다른 분포를 가진 난수 텐서들을 생성하는 여러가지 연산들이 있다.\n",
    "\n",
    "   난수 연산들은 상태를 가지며, 계산될 때마다 새로운 난수를 생성한다.\n",
    "\n",
    "- 이러한 함수들의 'seed' 키워드 인자는 그래프 수준의 난수 시드값과 함께 작용합니다.\n",
    "\n",
    "- set_random_seed(seed): 그래프 수준의 난수 시드를 설정한다.\n",
    "\n",
    " \n",
    "\n",
    "● Variable \n",
    "\n",
    "- tf.Variable()\n",
    "\n",
    "  : 변수 생성 (tensor를 메모리에 저장하는 변수)\n",
    "\n",
    "  ex) tf.Variable((tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "  *텐서플로우에서 변수형은 그래프를 실행하기 전에 초기화를 해줘야 그 값이 변수에 지정된다.\n",
    "\n",
    " x = tf.constant(1)\n",
    " y = tf.Variable(x+2)\n",
    " \n",
    "...\n",
    "sess=tf.Session()\n",
    "sess.run(y)  # 오류\n",
    "...\n",
    "\n",
    "init = tf.global_variables_initializer() # 변수 초기화 먼저 해주기\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(y)\n",
    "\n",
    "#출력: 3\n",
    "- tf.get_variable(<name>, <shape>,<dtype>,<initializer>)\n",
    "\n",
    "  : dtype 타입이고 이름이 name인 변수가 initializer(shape) 값으로 초기화되어 생성된다. \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "- tf.variable_scope(<scope_name>): tf.get_variable()에 전달된 name의 namespace를 관리함\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● feeding\n",
    "\n",
    "- feed_dict\n",
    "\n",
    "inputs=np.arange(10) # inputs=[0,1,2,3,4,5,6,7,8,9]\n",
    "a=tf.placeholder(dtype=tf.float32)\n",
    "b=a+10\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(b,feed_dict={a:inputs})\n",
    "# a에 순차적으로 inputs 값을 넣고, 값이 들어간 a를 가지고 연산 후 b에 값 넣음\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● 행렬 곱\n",
    "\n",
    "- tf.matmul(A,B): A,B가 행렬\n",
    "\n",
    " \n",
    "\n",
    "● 활성 함수\n",
    "\n",
    "- tf.nn.relu(L): 기존의 linear 함수인 sigmoid를 개선한 것\n",
    "\n",
    "  max(0, x)처럼 음수에 대해서만 0으로 처리하는 함수\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● softmax\n",
    "\n",
    "- softmax()\n",
    "\n",
    ": 입력을 0~1 사이의 값으로 변환한다. 변환된 결과의 전체 합이 1이 된다. -> 확률\n",
    "\n",
    "  ex) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● axis 인수\n",
    "\n",
    "- 연산의 대상이 2차원 이상일 경우, 어느 차원으로 계산 할 지 axis 인수로 결정\n",
    "\n",
    "  axis=0: 열 연산 / axis=1: 행 연산\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● reduce_\n",
    "\n",
    "- reduce_mean(): 텐서에 존재하는 모든 값의 평균\n",
    "\n",
    "- reduce_sum(): 텐서에 존재하는 모든 값의 평균 (차원 달라도 다 더함) \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● RNN - 정적으로 time step 펼치기 \n",
    "\n",
    "BasicRNNCell\n",
    "\n",
    "- tf.contrib.rnn.BasicRNNCell()\n",
    "\n",
    "- 셀의 복사본을 만드는 팩토리\n",
    "\n",
    " \n",
    "\n",
    "static_rnn()\n",
    "\n",
    "- 셀을 연결하여 펼쳐진 RNN 네트워크를 만든다.\n",
    "\n",
    "- 입력마다 셀의 __call__() 함수를 호출하여 가중치와 편향을 공유하는 셀 복사본 두 개를 만들고 서로 연결한다\n",
    "\n",
    "n_inputs =3\n",
    "n_neurons =5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 =tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "basic_cell =tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states =tf.contrib.rnn.static_rnn(basic_cell, [X0,X1], dtype=tf.float32)\n",
    "\n",
    "Y0,Y1 = output_seqs\n",
    " \n",
    "\n",
    "● transpose\n",
    "\n",
    "- tf.transpose(<바꿀 행렬>,<perm=>)\n",
    "\n",
    "- perm대로 <바꿀 행렬>의 차원을 바꿈\n",
    "\n",
    "- ex) X_seqs=tf.unstack(tf.transpose(X,perm=[1,0,2]))\n",
    "\n",
    " \n",
    "\n",
    "● unstaack\n",
    "\n",
    "- tf.unstack()\n",
    "\n",
    "- 첫 번째 차원을 따라 텐서의 파이썬 리스트를 추출한다. (타임 스텝마다 하나의 텐서)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● dynamic_rnn()\n",
    "\n",
    "- 적절한 타임 스텝에 걸쳐 셀을 실행하기 위해 while_loop() 연산을 사용한다.\n",
    "\n",
    "- backpropagation 시에 OOM 에러를 피하고 싶다면 swap_memory=True 옵션 설정\n",
    "\n",
    "  (GPU 메모리에서 CPU 메모리로 바꿈)\n",
    "\n",
    "- 각 타임 스텝의 모든 입력에 대해 텐서 하나를 받고([None, n_steps,n_inputs]),\n",
    "\n",
    "  타임 스텝마다 모든 출력을 하나의 텐서([None, n_steps, n_neurons])로 반환한다. \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● sparse_softmax_cross_entropy_with_logits()\n",
    "\n",
    "- tf.nn.sparse_softmax_cross_entropy_with_logits(<labels><logits>)\n",
    "\n",
    "- 비용함수 정의하기 위해 쓰임\n",
    "\n",
    "- 크로스 엔트로피는 모델이 타깃 클래스에 대해 낮은 확률을 추정하지 않도록 제약을 가한다.\n",
    "\n",
    "- logit(softmax 활성화 함수로 들어가기 전의 네트워크 출력)을 기반으로 크로스 엔트로피 계산\n",
    "\n",
    "- 0~(클래스 수-1) 사이의 정수로 된 레이블 기대\n",
    "\n",
    "- 각 샘플에 대한 크로스 엔트로피를 담은 ID 텐서를 반환\n",
    "\n",
    "- labels_placeholder에서 1-hot label을 자동으로 생성하고\n",
    "\n",
    "- inference() 함수의 1-hot labels 출력 logits을 비교하기 위해 추가된다.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "● _in_top_k\n",
    "\n",
    "- tf.nn.in_top_k(<predictions>,<target>,<k>)\n",
    "\n",
    "- 예측값, 타깃 레이블을 입력받아 타깃 레이블의 예측값이 크기순으로 K번째 안에 들면 True, 그렇지 않으면 False 반환"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
